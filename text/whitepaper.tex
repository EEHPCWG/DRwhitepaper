\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage[pdftex]{hyperref}
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running  heads
%
%
%
%
\mainmatter              % start of the contributions
%
\title{
The Electrical Grid and Supercomputer Centers:  
An Investigative Analysis of Emerging Opportunities and Challenges
}

%
\titlerunning{Electrical Grid}  % abbreviated title (for running head)
% also used for the TOC unless 
% \toctitle is used
%
%
\author{Fname Lname\inst{1} \and Fname Lname\inst{2}
Fname Lname \and Fname Lname \and Fname Lname \and Fname~M.~Lname \and
Fname Lname}
%
\authorrunning{Fname Lname et al.} % abbreviated author list (for running head)
%
%
\institute{Princeton University, Princeton NJ 08544, USA,\\
\email{I.Ekeland@princeton.edu},\\ WWW home page:
\texttt{http://users/\homedir iekeland/web/welcome.html}
\and
Universit\'{e} de Paris-Sud,
Laboratoire d'Analyse Num\'{e}rique, B\^{a}timent 425,\\
F-91405 Orsay Cedex, France}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Some of the largest supercomputer centers in the United 
States are developing new relationships with their 
electricity providers.  These relationships are driven by 
mutual interest.  Supercomputer centers are concerned about 
electricity price, quality, environmental impact and 
availability. Electricity providers are concerned about 
supercomputer center's impact on the electrical grid, both 
for energy consumption, peak power and fluctuations in 
power. Supercomputer center power demand can be greater 
than 20 megawatts (MW), theoretical peak power requirements 
greater than 45MW and re-occurring intra-hour variability 
can exceed 8MW. As a consequence, there are some 
supercomputer centers whose electricity providers are 
asking for hourly forecasts of power demand, a day in 
advance.   This paper explores today's relationships, 
potential partnerships and possible integration between 
supercomputer centers and their electricity providers. It 
develops a model for possible integration between 
supercomputer centers and the electrical grid. It then 
explores the utility of this model based on feedback from a 
questionnaire of Top 100 List sized supercomputer centers 
in the United States.
\end{abstract}

%
\section{Introduction}
%
Supercomputer centers with petascale systems for high-performance 
computing (HPC) are realizing the large impact they will be 
putting on their electricity service providers as they bring on 
(and perhaps turnoff or idle) megawatt scale (some double digit) supercomputers.
 
The Energy Efficient HPC Working Group 
(\href {http://eehpcwg.lbl.gov/}{EE HPC WG}) 
has been investigating opportunities for large supercomputing sites to more closely 
integrate with their electricity service providers. This paper documents the 
results of this investigative activity. 

Leveraging prior work on data center and grid integration opportunities done
by Lawrence Berkeley National Laboratory's Demand Response Research Center
(\href
{http://drrc.lbl.gov/publications/demand-response-and-open-automated-demand-response-opportunities-data-centers}{http://drrc.lbl.gov/publications}),
 this paper takes as a starting point LBNL's model for integrating data
centers and the electrical grid. The model describes programs that are used
by the electricity service providers to integrate with their customers (such
as demand response) and methods used to balance the grid supply and demand
of electricity. It also describes strategies that data centers might employ
for managing their electricity and power requirements. This paper tuned this
model's data center strategies for supercomputer centers.

The first section of this paper describes in greater detail the model for
integrating supercomputer centers and the electrical grid. The second
section is a review of prior work on HPC center strategies that might be
deployed for managing electricity and power. In order to further understand
today's relationships, potential partnerships and possible integration
between HPC centers, their electricity providers and the grid, a
questionnaire was deployed whose respondents were Top100 List class
supercomputer centers in the United States. The third section of this paper
describes the results of that questionnaire. The fourth section of the paper
describes opportunities, solutions and barriers. A fifth section describes
conclusions and next steps. Finally, the last section recognizes additional

%
\section{Supercomputing Centers and Electrical Grid Integration}
%
The EE HPC WG Team took as their starting point a model developed by
Lawrence Berkeley National Laboratory's Demand Reponse Research Center
\href{http://drrc.lbl.gov/publications/demand-response-and-open-automated-demand-response-opportunities-data-centers}
{http://drrc.lbl.gov/publications}
 that describes ways in which data centers and electricity service
providers may interact. This model describes programs that are used by the
electricity service providers to encourage particular behaviors by their
customers and methods used to balance the grid supply and demand of
electricity. It also describes strategies that data centers might employ for
managing their electricity and power requirements. The EE HPC WG Team
adopted this model with slight tweaks to reflect the HPC environment (versus
the general data center).

\subsection{Electricity Provider Programs and Methods}
Programs are used by the electricity service providers to encourage
particular behaviors by their customers. Methods used to balance the grid
supply and demand of electricity.

Below is a list of programs and methods:


\begin{itemize}
\item Energy Efficiency: Programs used to reduce overall electricity consumption, generally but not always at times of
 peak demand.
\item Peak Shaving (shed): Programs used to reduce load during peak times, where the reduced load is not used at a 
later time.
\item Peak Shaving (shift): Programs where the load during peak times is moved to, typically, non-peak hours.
\item Dynamic Pricing: Time varying pricing programs used to increase, shed or shift electricity consumption.
\item Grid Scale Storage: Methods used to store electricity on a large scale. Pumped-storage hydroelectricity 
is the largest-capacity form of grid energy storage.
\item Renewable (off-site): Methods used to manage the variable uncertain generation nature of many renewable resources.
\item Frequency response: Methods used to keep grid frequency constant and in-balance. Generators are typically used for frequency response.
\item Regulation (Up or Down): Methods used to maintain that portion of electricity generation reserves 
that are needed to balance generation and demand at all times.
\item Congestion: Methods used to resolve congestion that occurs when there is not enough transmission capability to 
support all requests for transmission services. Or, methods used to resolve congestion that occurs when the distributio
\end{itemize}

\subsection{Supercomputer Center Strageties}

Another dimension of the model is a list of strategies that a supercomputer
site might use for managing power in response to a request from their
electric service provider.

Although these strategies can be used for managing power in response to a
request from an electric service provider, many of them could also be used
for improving energy efficiency. It is the former that is of primary
interest to this investigation. Two examples may help to clarify this
distinction. Load migration is an example of a strategy that is well suited
to responding to an electric service provider request, but is not likely to
improve energy efficiency. Fine grained power management, on the other hand,
is more likely to be used for improving energy efficiency than for
responding to electric service provider requests.

Below is a list of strategies:

\begin{itemize}
\item Fine grained power management refers to the ability to control HPC system power 
and energy with tools that are high resolution control and can target specific 
low level sub-systems. A typical example is voltage and frequency scaling of the CPU.

\item Course grained power management also refers to the ability to control HPC 
system power and energy, but contrasts with fine grained power management in 
that the resolution is low and it is generally done at a more aggregated level. 
A typical example is power capping.

\item Load migration refers to temporarily shifting computing loads from 
an HPC system in one site to a system in another location.

\item Job scheduling refers to the ability to control HPC system power 
by understanding the power profile of applications and queuing the 
applications based on those profiles.

\item Back-up scheduling refers to deferring data storage processes to off-peak periods.

\item Shutdown refers to a graceful shutdown of idle HPC equipment. It usually 
applies when there is redundancy.

\item Lighting control allows for data center lights to be shutdown completely.

\item Thermal management is widening temperature set-point ranges and 
humidity levels for short periods.
\end{itemize}

\section{Prior Work}

The prior work described in this paper is that which addresses strategies
that HPC centers can take to manage power. A lot of work has been done on
energy efficiency, some of which has an element of power management. But,
there is not a lot work that is specifically focused on power management in
response to a request from an electrical service provider.

\subsection{Fine Grained Power Management}
In (Varma et al., 2003) system level DVFS techniques demonstrated. They
monitor CPU utilization at regular intervals and then perform dynamic
scaling based on their estimate of utilization for the next interval.

Authors in (von Laszewski et al., 2009) present an efficient scheduling
algorithm to allocate virtual machines in a DVFS-enabled cluster by
dynamically scaling the supplied voltages.

vGreen (Dhiman et al., 2009) is a system for energy efficient computing in
virtualized environments by linking online workload characterization to
dynamic VM scheduling decisions to achieve better performance, energy
efficiency and power balance in the system.

\subsection{Power Capping}

\textbf{Power capping }techniques set a value below the actual peak power
and preventing that number from being exceeded through some type of control
loop [Fan07]. There are numerous ways to implement this, but they generally
consist of a \textbf{power monitoring system }such as a power estimation
method or one based on direct power sensing, and a \textbf{power throttling
mechanism}. Power throttling generally works best when there is \textbf{a
set of jobs with loose service level guarantees or low priority }that can be
forced to reduce consumption when the data center is approaching \textbf{the
power cap value}. Power consumption can be reduced simply by
\textbf{de-scheduling tasks }or by using any available component-level power
management \textbf{knobs}, such as DVFS [Fan07].

\subsection{Job Scheduling}
The problem of scheduling jobs has been extensively studied. In general,
most of the schedulers implement the First Come First Served (FCFS) policy
as a simple but fair strategy for scheduling jobs. But this policy suffers
from low system utilization. The most commonly used optimization is backfill
[Lif95, Mua95, Fei04]. Backfilling is proposed to improve the system
utilization. Backfilling by identifying free capacities allows the smaller
jobs fit those capacities to move forward and run on idle processors.

In [Yang13] and [Zhou13], \textbf{job scheduling} as a DR strategy and
\textbf{dynamic pricing} as a grid integration program have been used to
propose \textbf{a power-aware job scheduling} approach to reduce
\textbf{electricity costs} \textit{without degrading the system utilization}. 
The novelty of the proposed job scheduling
mechanism is its ability to take \textit{the variation of 
electricity price }into consideration as a means to make
better decisions of the timing of scheduling jobs with diverse power
profiles. Experimentations on an IBM Blue Gene/P and a cluster system as
well as a case study on Argonne's 48-rack IBM Blue Gene/Q system have
demonstrated the effectiveness of this scheduling approach. Preliminary
results show a \textbf{23{\%}} reduction in electricity cost of HPC systems.

A grid computing infrastructure with large amount of computations normally
contains parallel machines (supercomputers cluster) as main computational
resources [Fos01]. Incoming jobs to Grid's local resources are scheduled by
local scheduling system. Local scheduling system for parallel machines
typically use batch queued space-sharing and its variants as scheduling
policies. Most current local schedulers use backfilling strategies with FCFS
queue-priority order as policy for parallel job scheduling. In the US,
supercomputer centers are connected via grid computing infrastructures such
as TeraGrid, Open Science Grid. Grid computing's protocols, interfaces, and
standards can facilitate the execution of DR strategies, as a result grid
computing may increase the interest level and/or the impact level of DR
strategies.

There are many use cases in grid computing environment that require QoS
guarantees in terms of guaranteed response time, including time-critical
tasks that must meet a deadline, which would be impossible without a start
time guarantee. Furthermore providing time guarantee enable the job to be
coordinated with other activities, essential for co-allocation and workflows
applications. Advance reservation is a guarantee for the availability of a
certain amount of resources to users and applications at specific times in
the future [Fos99]. Advance reservation feature requires local scheduling
systems to support a reservation capability beside batch queued policy for
local and normal jobs. In load migration, we encounter the need to deliver
resources at specific times in order to accept jobs from other HPC centers
to respond to their demand enforced by electricity grid. This requirement
can be achieved by advance reservations [Fos99]. Modern resource management
and scheduling systems such as Sun Grid Engine, PBS, OpenPBS, Torque, Maui,
and Moab support backfilling and advance reservation capabilities.

By using \textit{advance reservation capabilities }of schedulers 
(within local resource managers) of HPC centers, we
facilitate \textit{the execution of load migration strategy }between 
HPC centers (e.g., in terms of automation); as a result
we increase the interest level and to some extent the impact level of load
migration strategies.

\subsection{Load Migration}

In order to balance the electrical grid, [Chiu12] proposes a low-cost
\textbf{geographic load migration} to match electricity supply. In addition,
authors present a real grid balancing problem experienced in the Pacific
Northwest. They propose a symbiotic relationship between data centers and
electrical grid operators by showing that \textbf{mutual cost benefits }can
be accessible.

\subsection{Thermal Management}
Thermal and cooling metrics are becoming important metrics in scheduling and
resource management of HPC centers. Runtime cooling strategies are mostly
job-placement-centric. These techniques either aim to place incoming
computationally intensive jobs in a thermal-aware manner on servers with
lower temperatures or attempt to reactively migrate/load-balance jobs from
high temperature servers to servers with lower temperatures.
\textbf{\textit{T* }}[Kau12] takes a data-centric thermal- and
energy-management approach and does proactive, thermal-aware file placement
which allows cooling energy costs savings without performance trade-offs. T*
is cognizant of the uneven thermal-profile of the servers, differences in
their thermal-reliability-driven load thresholds, and differences in the
data-semantics, i.e., computation job rates, sizes, and evolution life
spans, of the big data placed in the cluster.

In this paper, we assume that the grid is a given constant as a fundamental
property. But, grid integration solutions may take into consideration that
it isn't a given as electrical grid infrastructures will evolve in the
future [He08]. Thus, changes in the grid could make grid integration more or
less difficult.

In [Aik11], authors explored the potential for HPC centers to adapt to
dynamic electrical prices, variation in carbon intensity within an
electrical grid, or availability of local renewables. Through simulations
experiments on workloads from the Parallel Workloads Archive alongside
real-world pricing data, they demonstrate potential savings on the cost of
electricity ranging typically between 10-50{\%}. Nonetheless, adaptation to
the variation in the electrical grid carbon intensity was not as successful,
but adaptation to the availability of local renewables showed potential to
significantly increase their use.

\section{Questionaire} 
We used a questionnaire in order to understand the current experiences of
supercomputer centers with respect to interacting with their electricity
service providers. We restricted the analysis to sites in the United States
since the results of the survey and practices of demand response is highly
correlated and driven by energy policies in the country. [Tor10].

Nineteen Top100 List sized sites in the United States were targeted for the
questionnaire. Eleven sites responded (ORNL, LLNL, ANL, LANL, LBNL, WPAFB,
NOAA, NCSA, SDSC, Purdue, and Intel) and eight sites didn't respond (NCAR,
IBM, NETL, Indiana University, TACC, SNL, NREL, NASA). The questionnaire was
sent to a sample that was not randomly selected. It was sent to those sites
where it was relatively easy to identify an individual based on membership
within the EE HPC WG. The sample is more representative of Top50 sized sites
(1 Top50 sized site was not in the sample and 60{\%} (9/15) of the sample
responded). Only 4 additional sites were sampled from the Top51-Top100 List
and, of those, 2 responded (Intel and NOAA).

The total power load as well as the intra-hour fluctuation of these sites
varied significantly. There were four sites with total power load greater
than 10MW, two sites with \textasciitilde 5MW total power load and five
sites with less than 2MW total power load. We chose less than 3MW intra hour
variability as the bottom of the scale because we assumed that the
electrical service providers would not be affected by that magnitude of
fluctuation. For those with total power load greater than 10MW, the
intra-hour fluctuation varied from less than 3MW to 8MW. One of
\textasciitilde 5MW sites said that they experienced 4MW variability. The
rest of the sites were all less than 3MW. Most of the intra-hour variability
was due to preventative maintenance.





\begin{table}[htbp]
\begin{center}
\begin{tabular}{|p{65pt}|l|l|}
\hline
\textbf{Total Load}&
\textbf{Variability}&
\textbf{Frequency} \\
\hline
16-17MW&
5MW&
weekly \\
\hline
13-14MW&
8MW&
monthly \\
\hline
10-11MW&
Less than 3MW&
weekly \\
\hline
10-11MW&
7MW&
weekly \\
\hline
4-5MW&
Less than 3MW&
weekly \\
\hline
4-5MW&
4MW&
weekly \\
\hline
1-2MW&
Less than 3MW&
weekly \\
\hline
1-2MW&
140kW&
daily \\
\hline
1-2MW&
Less than 3MW&
yearly \\
\hline
1-2MW&
200kW or less&
daily \\
\hline
1-2MW&
Less than 3MW&
daily \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

We asked if the supercomputer centers had talked to their electric service
providers about programs and methods used to balance the grid supply and
demand of electricity. About half of them have had some discussion, but it
has mostly been limited to programs and not methods.

\begin{table}[htbp]
\begin{center}
\begin{tabular}{|p{230pt}|l|}
\hline
\textbf{Discussions with Electricity Providers}&
{\%} Answered Yes \\
\hline
\textbf{Programs}&
~ \\
\hline
Shedding load during peak demand&
54 \\
\hline
Responding to pricing incentive programs&
45 \\
\hline
Shifting load during peak demand&
36 \\
\hline
\textbf{Methods}&
~ \\
\hline
Enabling use of renewables&
36 \\
\hline
Congestion, Regulation, Frequency Response&
18 \\
\hline
Contributing to electrical grid storage&
10 \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}


More than half of the respondents are not interested in shedding or shifting
load during peak demand. There is some indication that this low interest is
primarily due to the lack of a clear business case. For the sites where
there is interest, shifting is more attractive than shedding load. SDSC is
an exception to this trend, but because of a site-wide program. ``UCSD
generates 30-35MW of power yet still imports 5-10MW. As a large generation
source the utility providers see the campus as a highly attractive partner
for offloading grid stress. Automatic load shedding is being
explored/deployed today.''

Responding to pricing incentive programs is also not considered interesting,
although the reasons for this low interest may be organizational. Several
open-ended comments revealed that pricing is fixed and/or done by another
organization at the site level and outside of their immediate control.

Eighty percent of the respondents have not had discussions with their
electricity service providers about congestion, regulation and frequency
response. LANL is one of two who have had discussions and who commented that
they are ``learning about the process'' and that it is ``outside of [their]
visibility or control''.

There were been many more respondents who have had discussions with their
electricity service providers about enabling the use of renewables; 36{\%}
have already had discussions and more than half are interested in further
and/or future discussions. SDSC already has a site-wide program; ``the
campus has a large fuel cell (2.5$+$ MW) and works with the utility with
renewables.'' Other responses suggest that the interest is at the site level
and not unique to the supercomputer center.

An open-ended question was posed as to whether or not there was information
either requested of the supercomputer sites by their providers or,
conversely, requested of the providers by the sites. In both cases, well
over 75{\%} of the respondents answered no. LLNL and LANL were the
exceptions. LLNL is ``working on obtaining additional data from them and a
means of sharing data between them and us'' and has been requested to
provide ``additional detailed forecasting and ultimately real time data.''
LANL has also been requested to provide ``power projections, hour by hour,
for at least a day in advance'' and, perhaps as a consequence, would like to
have more information on ``sensitivity of power distribution grid to rapid
transients (random daily step changes of 10 MW up or down within a single AC
cycle).''

Given the low levels of current engagement between the electricity service
providers and the supercomputer centers, it is not surprising that none of
the supercomputer centers are currently using any power management
strategies to respond to grid requests by their electrical service
providers. SDSC's \textit{supercomputer center} is not an exception, but they did respond that their
entire ``campus is leveraging parallel electrical distribution to trigger
diesel generators and other back-up resources to respond to to grid and
non-grid requests.''

We tried to evaluate if power management strategies will be considered
relevant and effective for grid integration at some point in the future. Two
questions were asked; is there interest in using the strategies and what
impact did they think that the strategies would have. When combining
interest and impact, the results showed that power capping, shutdown, and
job scheduling were both high interest and impact. Load migration, back-up
scheduling, fine grained power management and thermal management were medium
interest and impact. Lighting control and back-up resources were low
interest and impact.
==============================

\begin{table}
\caption{This is the example table taken out of {\it The
\TeX{}book,} p.\,246}
\begin{center}
\begin{tabular}{r@{\quad}rl}
\hline
\multicolumn{1}{l}{\rule{0pt}{12pt}
                   Year}&\multicolumn{2}{l}{World 
population}\\[2pt]
\hline\rule{0pt}{12pt}
8000 B.C.  &     5,000,000& \\
  50 A.D.  &   200,000,000& \\
1650 A.D.  &   500,000,000& \\
1945 A.D.  & 2,300,000,000& \\
1980 A.D.  & 4,400,000,000& \\[2pt]
\hline
\end{tabular}
\end{center}
\end{table}
%

%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
%
\bibitem {clar:eke}
Clarke, F., Ekeland, I.:
Nonlinear oscillations and
boundary-value problems for Hamiltonian systems.
Arch. Rat. Mech. Anal. 78, 315--333 (1982)

\bibitem {clar:eke:2}
Clarke, F., Ekeland, I.:
Solutions p\'{e}riodiques, du
p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
Note CRAS Paris 287, 1013--1015 (1978)

\bibitem {mich:tar}
Michalek, R., Tarantello, G.:
Subharmonic solutions with prescribed minimal
period for nonautonomous Hamiltonian systems.
J. Diff. Eq. 72, 28--55 (1988)

\bibitem {tar}
Tarantello, G.:
Subharmonic solutions for Hamiltonian
systems via a $\bbbz_{p}$ pseudoindex theory.
Annali di Matematica Pura (to appear)

\bibitem {rab}
Rabinowitz, P.:
On subharmonic solutions of a Hamiltonian system.
Comm. Pure Appl. Math. 33, 609--633 (1980)

\end{thebibliography}

\end{document}
