\documentclass{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\usepackage[pdftex]{hyperref}
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running  heads
%
%
%
%
\mainmatter              % start of the contributions
%
\title{
The Electrical Grid and Supercomputer Centers:  
An Investigative Analysis of Emerging Opportunities and Challenges
}

%
\titlerunning{Electrical Grid}  % abbreviated title (for running head)
% also used for the TOC unless 
% \toctitle is used
%
%
\author{Fname Lname\inst{1} \and Fname Lname\inst{2}
Fname Lname \and Fname Lname \and Fname Lname \and Fname~M.~Lname \and
Fname Lname}
%
\authorrunning{Fname Lname et al.} % abbreviated author list (for running head)
%
%
\institute{Princeton University, Princeton NJ 08544, USA,\\
\email{I.Ekeland@princeton.edu},\\ WWW home page:
\texttt{http://users/\homedir iekeland/web/welcome.html}
\and
Universit\'{e} de Paris-Sud,
Laboratoire d'Analyse Num\'{e}rique, B\^{a}timent 425,\\
F-91405 Orsay Cedex, France}

\maketitle              % typeset the title of the contribution

\begin{abstract}
Some of the largest supercomputer centers in the United 
States are developing new relationships with their 
electricity providers.  These relationships are driven by 
mutual interest.  Supercomputer centers are concerned about 
electricity price, quality, environmental impact and 
availability. Electricity providers are concerned about 
supercomputer center's impact on the electrical grid, both 
for energy consumption, peak power and fluctuations in 
power. Supercomputer center power demand can be greater 
than 20 megawatts (MW), theoretical peak power requirements 
greater than 45MW and re-occurring intra-hour variability 
can exceed 8MW. As a consequence, there are some 
supercomputer centers whose electricity providers are 
asking for hourly forecasts of power demand, a day in 
advance.   This paper explores today's relationships, 
potential partnerships and possible integration between 
supercomputer centers and their electricity providers. It 
develops a model for possible integration between 
supercomputer centers and the electrical grid. It then 
explores the utility of this model based on feedback from a 
questionnaire of Top 100 List sized supercomputer centers 
in the United States.
\end{abstract}

%
\section{Introduction}
%
Supercomputer centers with petascale systems for high-performance 
computing (HPC) are realizing the large impact they will be 
putting on their electricity service providers as they bring on 
(and perhaps turnoff or idle) megawatt scale (some double digit) supercomputers.
 
The Energy Efficient HPC Working Group 
(\href {http://eehpcwg.lbl.gov/}{EE HPC WG}) 
has been investigating opportunities for large supercomputing sites to more closely 
integrate with their electricity service providers. This paper documents the 
results of this investigative activity. 

Leveraging prior work on data center and grid integration opportunities done
by Lawrence Berkeley National Laboratory's Demand Response Research Center
(\href
{http://drrc.lbl.gov/publications/demand-response-and-open-automated-demand-response-opportunities-data-centers}{http://drrc.lbl.gov/publications}),
 this paper takes as a starting point LBNL's model for integrating data
centers and the electrical grid. The model describes programs that are used
by the electricity service providers to integrate with their customers (such
as demand response) and methods used to balance the grid supply and demand
of electricity. It also describes strategies that data centers might employ
for managing their electricity and power requirements. This paper tuned this
model's data center strategies for supercomputer centers.

The first section of this paper describes in greater detail the model for
integrating supercomputer centers and the electrical grid. The second
section is a review of prior work on HPC center strategies that might be
deployed for managing electricity and power. In order to further understand
today's relationships, potential partnerships and possible integration
between HPC centers, their electricity providers and the grid, a
questionnaire was deployed whose respondents were Top100 List class
supercomputer centers in the United States. The third section of this paper
describes the results of that questionnaire. The fourth section of the paper
describes opportunities, solutions and barriers. A fifth section describes
conclusions and next steps. Finally, the last section recognizes additional

%
\section{Supercomputing Centers and Electrical Grid Integration}
%
The EE HPC WG Team took as their starting point a model developed by
Lawrence Berkeley National Laboratory's Demand Reponse Research Center
\href{http://drrc.lbl.gov/publications/demand-response-and-open-automated-demand-response-opportunities-data-centers}
{http://drrc.lbl.gov/publications}
 that describes ways in which data centers and electricity service
providers may interact. This model describes programs that are used by the
electricity service providers to encourage particular behaviors by their
customers and methods used to balance the grid supply and demand of
electricity. It also describes strategies that data centers might employ for
managing their electricity and power requirements. The EE HPC WG Team
adopted this model with slight tweaks to reflect the HPC environment (versus
the general data center).

\subsection{Electricity Provider Programs and Methods}
Programs are used by the electricity service providers to encourage
particular behaviors by their customers. Methods used to balance the grid
supply and demand of electricity.

Below is a list of programs and methods:


\begin{itemize}
\item Energy Efficiency: Programs used to reduce overall electricity consumption, generally but not always at times of
 peak demand.
\item Peak Shaving (shed): Programs used to reduce load during peak times, where the reduced load is not used at a 
later time.
\item Peak Shaving (shift): Programs where the load during peak times is moved to, typically, non-peak hours.
\item Dynamic Pricing: Time varying pricing programs used to increase, shed or shift electricity consumption.
\item Grid Scale Storage: Methods used to store electricity on a large scale. Pumped-storage hydroelectricity 
is the largest-capacity form of grid energy storage.
\item Renewable (off-site): Methods used to manage the variable uncertain generation nature of many renewable resources.
\item Frequency response: Methods used to keep grid frequency constant and in-balance. Generators are typically used for frequency response.
\item Regulation (Up or Down): Methods used to maintain that portion of electricity generation reserves that are needed to balance generation and demand at all times.
\item Congestion: Methods used to resolve congestion that occurs when there is not enough transmission capability to 
support all requests for transmission services. Or, methods used to resolve congestion that occurs when the distributio
\end{itemize}

\subsection{Supercomputer Center Strageties}

Another dimension of the model is a list of strategies that a supercomputer
site might use for managing power in response to a request from their
electric service provider.

Although these strategies can be used for managing power in response to a
request from an electric service provider, many of them could also be used
for improving energy efficiency. It is the former that is of primary
interest to this investigation. Two examples may help to clarify this
distinction. Load migration is an example of a strategy that is well suited
to responding to an electric service provider request, but is not likely to
improve energy efficiency. Fine grained power management, on the other hand,
is more likely to be used for improving energy efficiency than for
responding to electric service provider requests.

Below is a list of strategies:

\begin{itemize}
\item Fine grained power management refers to the ability to control HPC system power 
and energy with tools that are high resolution control and can target specific 
low level sub-systems. A typical example is voltage and frequency scaling of the CPU.

\item Course grained power management also refers to the ability to control HPC 
system power and energy, but contrasts with fine grained power management in 
that the resolution is low and it is generally done at a more aggregated level. 
A typical example is power capping.

\item Load migration refers to temporarily shifting computing loads from 
an HPC system in one site to a system in another location.

\item Job scheduling refers to the ability to control HPC system power 
by understanding the power profile of applications and queuing the 
applications based on those profiles.

\item Back-up scheduling refers to deferring data storage processes to off-peak periods.

\item Shutdown refers to a graceful shutdown of idle HPC equipment. It usually 
applies when there is redundancy.

\item Lighting control allows for data center lights to be shutdown completely.

\item Thermal management is widening temperature set-point ranges and 
humidity levels for short periods.
\end{itemize}

\section{Prior Work}

The prior work described in this paper is that which addresses strategies
that HPC centers can take to manage power. A lot of work has been done on
energy efficiency, some of which has an element of power management. But,
there is not a lot work that is specifically focused on power management in
response to a request from an electrical service provider.

\subsection{Fine Grained Power Management}
In (Varma et al., 2003) system level DVFS techniques demonstrated. They
monitor CPU utilization at regular intervals and then perform dynamic
scaling based on their estimate of utilization for the next interval.

Authors in (von Laszewski et al., 2009) present an efficient scheduling
algorithm to allocate virtual machines in a DVFS-enabled cluster by
dynamically scaling the supplied voltages.

vGreen (Dhiman et al., 2009) is a system for energy efficient computing in
virtualized environments by linking online workload characterization to
dynamic VM scheduling decisions to achieve better performance, energy
efficiency and power balance in the system.

\subsection{Power Capping}

\textbf{Power capping }techniques set a value below the actual peak power
and preventing that number from being exceeded through some type of control
loop [Fan07]. There are numerous ways to implement this, but they generally
consist of a \textbf{power monitoring system }such as a power estimation
method or one based on direct power sensing, and a \textbf{power throttling
mechanism}. Power throttling generally works best when there is \textbf{a
set of jobs with loose service level guarantees or low priority }that can be
forced to reduce consumption when the data center is approaching \textbf{the
power cap value}. Power consumption can be reduced simply by
\textbf{de-scheduling tasks }or by using any available component-level power
management \textbf{knobs}, such as DVFS [Fan07].

\subsection{Job Scheduling}
The problem of scheduling jobs has been extensively studied. In general,
most of the schedulers implement the First Come First Served (FCFS) policy
as a simple but fair strategy for scheduling jobs. But this policy suffers
from low system utilization. The most commonly used optimization is backfill
[Lif95, Mua95, Fei04]. Backfilling is proposed to improve the system
utilization. Backfilling by identifying free capacities allows the smaller
jobs fit those capacities to move forward and run on idle processors.

In [Yang13] and [Zhou13], \textbf{job scheduling} as a DR strategy and
\textbf{dynamic pricing} as a grid integration program have been used to
propose \textbf{a power-aware job scheduling} approach to reduce
\textbf{electricity costs} \textit{without degrading the system utilization}. 
The novelty of the proposed job scheduling
mechanism is its ability to take \textit{the variation of 
electricity price }into consideration as a means to make
better decisions of the timing of scheduling jobs with diverse power
profiles. Experimentations on an IBM Blue Gene/P and a cluster system as
well as a case study on Argonne's 48-rack IBM Blue Gene/Q system have
demonstrated the effectiveness of this scheduling approach. Preliminary
results show a \textbf{23{\%}} reduction in electricity cost of HPC systems.

A grid computing infrastructure with large amount of computations normally
contains parallel machines (supercomputers cluster) as main computational
resources [Fos01]. Incoming jobs to Grid's local resources are scheduled by
local scheduling system. Local scheduling system for parallel machines
typically use batch queued space-sharing and its variants as scheduling
policies. Most current local schedulers use backfilling strategies with FCFS
queue-priority order as policy for parallel job scheduling. In the US,
supercomputer centers are connected via grid computing infrastructures such
as TeraGrid, Open Science Grid. Grid computing's protocols, interfaces, and
standards can facilitate the execution of DR strategies, as a result grid
computing may increase the interest level and/or the impact level of DR
strategies.

There are many use cases in grid computing environment that require QoS
guarantees in terms of guaranteed response time, including time-critical
tasks that must meet a deadline, which would be impossible without a start
time guarantee. Furthermore providing time guarantee enable the job to be
coordinated with other activities, essential for co-allocation and workflows
applications. Advance reservation is a guarantee for the availability of a
certain amount of resources to users and applications at specific times in
the future [Fos99]. Advance reservation feature requires local scheduling
systems to support a reservation capability beside batch queued policy for
local and normal jobs. In load migration, we encounter the need to deliver
resources at specific times in order to accept jobs from other HPC centers
to respond to their demand enforced by electricity grid. This requirement
can be achieved by advance reservations [Fos99]. Modern resource management
and scheduling systems such as Sun Grid Engine, PBS, OpenPBS, Torque, Maui,
and Moab support backfilling and advance reservation capabilities.

By using \textit{advance reservation capabilities }of schedulers 
(within local resource managers) of HPC centers, we
facilitate \textit{the execution of load migration strategy }between 
HPC centers (e.g., in terms of automation); as a result
we increase the interest level and to some extent the impact level of load
migration strategies.

\subsection{Load Migration}

In order to balance the electrical grid, [Chiu12] proposes a low-cost
\textbf{geographic load migration} to match electricity supply. In addition,
authors present a real grid balancing problem experienced in the Pacific
Northwest. They propose a symbiotic relationship between data centers and
electrical grid operators by showing that \textbf{mutual cost benefits }can
be accessible.

\subsection{Thermal Management}
Thermal and cooling metrics are becoming important metrics in scheduling and
resource management of HPC centers. Runtime cooling strategies are mostly
job-placement-centric. These techniques either aim to place incoming
computationally intensive jobs in a thermal-aware manner on servers with
lower temperatures or attempt to reactively migrate/load-balance jobs from
high temperature servers to servers with lower temperatures.
\textbf{\textit{T* }}[Kau12] takes a data-centric thermal- and
energy-management approach and does proactive, thermal-aware file placement
which allows cooling energy costs savings without performance trade-offs. T*
is cognizant of the uneven thermal-profile of the servers, differences in
their thermal-reliability-driven load thresholds, and differences in the
data-semantics, i.e., computation job rates, sizes, and evolution life
spans, of the big data placed in the cluster.

In this paper, we assume that the grid is a given constant as a fundamental
property. But, grid integration solutions may take into consideration that
it isn't a given as electrical grid infrastructures will evolve in the
future [He08]. Thus, changes in the grid could make grid integration more or
less difficult.

In [Aik11], authors explored the potential for HPC centers to adapt to
dynamic electrical prices, variation in carbon intensity within an
electrical grid, or availability of local renewables. Through simulations
experiments on workloads from the Parallel Workloads Archive alongside
real-world pricing data, they demonstrate potential savings on the cost of
electricity ranging typically between 10-50{\%}. Nonetheless, adaptation to
the variation in the electrical grid carbon intensity was not as successful,
but adaptation to the availability of local renewables showed potential to
significantly increase their use.
==================


\begin{figure}
\vspace{2.5cm}
\caption{This is the caption of the figure displaying a 
white eagle and
a white horse on a snow field}
\end{figure}

\begin{table}
\caption{This is the example table taken out of {\it The
\TeX{}book,} p.\,246}
\begin{center}
\begin{tabular}{r@{\quad}rl}
\hline
\multicolumn{1}{l}{\rule{0pt}{12pt}
                   Year}&\multicolumn{2}{l}{World 
population}\\[2pt]
\hline\rule{0pt}{12pt}
8000 B.C.  &     5,000,000& \\
  50 A.D.  &   200,000,000& \\
1650 A.D.  &   500,000,000& \\
1945 A.D.  & 2,300,000,000& \\
1980 A.D.  & 4,400,000,000& \\[2pt]
\hline
\end{tabular}
\end{center}
\end{table}
%

%
% ---- Bibliography ----
%
\begin{thebibliography}{5}
%
\bibitem {clar:eke}
Clarke, F., Ekeland, I.:
Nonlinear oscillations and
boundary-value problems for Hamiltonian systems.
Arch. Rat. Mech. Anal. 78, 315--333 (1982)

\bibitem {clar:eke:2}
Clarke, F., Ekeland, I.:
Solutions p\'{e}riodiques, du
p\'{e}riode donn\'{e}e, des \'{e}quations hamiltoniennes.
Note CRAS Paris 287, 1013--1015 (1978)

\bibitem {mich:tar}
Michalek, R., Tarantello, G.:
Subharmonic solutions with prescribed minimal
period for nonautonomous Hamiltonian systems.
J. Diff. Eq. 72, 28--55 (1988)

\bibitem {tar}
Tarantello, G.:
Subharmonic solutions for Hamiltonian
systems via a $\bbbz_{p}$ pseudoindex theory.
Annali di Matematica Pura (to appear)

\bibitem {rab}
Rabinowitz, P.:
On subharmonic solutions of a Hamiltonian system.
Comm. Pure Appl. Math. 33, 609--633 (1980)

\end{thebibliography}

\end{document}
