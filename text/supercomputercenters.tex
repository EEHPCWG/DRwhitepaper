In November 2004, the Blue Gene/L system at Lawrence Livermore National Laboratory
became the fastest computer in the Top 500~\cite{Top500}, displacing the NEC Earth Simulator,
the previous champion. This change marked the transition from supercomputing gains based
on ever-higher-performance components to systems that comprised of far larger numbers of 
slow but energy-efficient components. However, total system power consumption continued to rise,
and we are now poised to begin a second transition to ``power-limited computing'' and ``power-aware computing''. The new
model has been exemplified by the US Department of Energy issuing guidance that the first
DOE exascale machine should not exceed 20 MW; effectively a $1000x$ performance improvement
with only a $3x$ increase in power. 

However, the problem is not as simple as provisioning 20 MW. Ultimately, SCs optimize for
performance per dollar, not performance per Watt, and flexibility in power consumption
can be expected to result in lower overall prices. Use of green technologies such as
wind and solar energy may also lead to cheaper but less predictable sources of power.
In addition, as described in Section~\ref{sec:ESPintegration}, ESPs may request a change in timing and/or magnitude of demand by SCs. To adapt to this new landscape, SCs may employ one or more strategies to control their electricity demand. 

The EE HPC WG took as their starting point a
model developed by LBNL's Demand Response Research
Center. This model describes strategies that datacenters might employ for utility programs to manage their electricity and
power requirements to lower costs and benefit from utility
incentives. The EE HPC WG adopted this model %with slight tweaks  TP:This makes the model seem trivial, which it is not.
to reflect the supercomputing environment
focus (as opposed to %TP: Replace 'versus' with 'as opposed to'
the datacenter focus described by LBNL`s Demand
Response Research Center).
%TP: Rephrasing the following sentence
%For purposes of this paper, we define SCs 
%as distinct from datacenters as having 
%significantly higher system utilization and thus little or no 
%virtualization.  

It is important to highlight the differences between SCs and datacenters. Unlike datacenters, SCs are more performance oriented, have significantly higher system utilization, and use little or no virtualization. 
Additionally, supercomputing
applications are distinguished by their lack of geographical
portability due to security concerns, data size and machine-specific
optimizations.  
%
We also note that SCs tend to be more
energy efficient than datacenters. Power Usage Effectiveness (PUE) is a good measure for energy efficiency. PUE is the ratio of the total energy supplied for the facility to the amount of energy that actually reaches the IT infrastructure. A PUE of $1.0$ is ideal. In our survey, none of the SCs exceeded a Power Usage Effectiveness (PUE) of $1.53$, while the average PUE for a datacenter falls in the range of $1.91$ and $2.9$ ~\cite{Niccolai}.

\subsection{Strategies}
%We describe some of these strategies below. 
We describe below some of the strategies that SCs may use to adapt to the changing landscape.

\begin{itemize}
\item {\bf Fine-grained Power Management} refers to the ability to control SC system power 
and energy with tools that offer high resolution control and can target specific 
low level sub-systems. A typical example is CPU voltage and frequency scaling.

\item {\bf Coarse-grained Power Management} also refers to the ability to control SC 
system power and energy, but contrasts with fine-grained power management in 
that the resolution is low and it is generally done at a more aggregated level. 
A typical example is power capping.

\item {\bf Load Migration} refers to temporarily shifting computing loads from 
an SC system in one site to a system in another location that has stable power supply. 
This strategy can also be used in response to change in electricity prices.

\item {\bf Job Scheduling} refers to the ability to control SC system power 
by understanding the power profile of applications and queuing the 
applications based on those profiles.

\item {\bf Back-up Scheduling} refers to deferring data storage processes to off-peak periods.

\item {\bf Shutdown} refers to a graceful shutdown of idle SC equipment. It usually 
applies when there is redundancy.

\item {\bf Lighting Control} allows for datacenter lights to be shutdown completely.

\item {\bf Thermal Management} is widening temperature set-point ranges and 
humidity levels for short periods.
\end{itemize}

These strategies can be used temporarily to modify loads in response to a request from an ESP. Additionally, some of these strategies could eventually be used at all times to improve overall energy efficiency if the SC sees no operational issues. Two examples may help to clarify this distinction. Temporary load migration is an example of a strategy that is well suited to responding to an ESP request, but is not likely to improve energy efficiency (lowering aggregate energy use). Fine-grained power management, on the other hand, can be used at all times and is more likely to be used for improving overall energy efficiency, unless the strategy is specifically used in response to an ESP's request. 

\subsection{Implementations}
SC system power management has a very broad range of implementations and warrants greater exploration. For example, the coarse-grained and fine-grained strategies described above can be implemented at many levels of the system hierarchy---from node-level to site-level. We discuss these implementation approaches below.  

\begin{itemize}
\item \textbf{Node level:} Controlling power ultimately requires control of individual
components. Historically, this control has been accomplished through Dynamic Voltage/Frequency Scaling (DVFS), which allows the processor to use a lower voltage at the cost of a slower clock frequency. Newer technologies such as Intel's Running Average Power Limit leverage DVFS to guarantee that a user-specified processor power bound will, on average, not be exceeded over the duration of a short time window. DVFS can also be found on accelerator components such as NVIDIA's Kepler GPGPU. Other efforts reduce DRAM power by optimizing reads and writes, thus allowing the memory to spend more time in a lower-power state. Several processor configuration options have indirect but significant effects on power consumption. For example, the choice of the number of cores to use, whether or not to enable hyperthreading, and the use of ``turbo'' modes will change the power/performance curve.

\item \textbf{Job level:} Each of the node-level controls requires a tradeoff between
power and performance. SC resources are typically oversubscribed, so degrading performance
to save power and energy ultimately results in less science getting done. However, at the 
job level, load imbalance provides opportunities to slow nodes that are off of the critical
path of execution without slowing the overall job execution time. Traditionally, load 
rebalancing strategies have focused on moving bytes around the job allocation. With 
power control, we can now rebalance power as well as work.

\item \textbf{System level:} While most SCs use time and space partitioning (where a node
only runs a single job at a time), there are still shared resources that must be managed
across jobs. Periodic checkpointing saves sufficient job state to a filesystem shared 
across jobs so that a job may be restarted from a recent point in case a fault occurs.
Because these checkpoints involve much more data motion than normal execution, power 
spikes can be observed at the node level (particularly DRAM), network, and filesystem.
These checkpoints may need to be coordinated across large jobs to prevent unnecessary
performance degradation.
%TP: Checkpointing results in power spikes because more data motion takes place -- you need to save the state of the application (memory) to the filesystem. CPR in SCs is hierarchical, there's local storage first, and then this is written to a central (or a few central) file system serves via the network. This may lead to congestion (leading to more network power) as well as more power being used on the node.

\item \textbf{Scheduler level:} Up through the system level, power control is evaluated
using the execution time of individual jobs. The scheduler optimizes for overall throughput
rather than individual job performance. At this point, scheduling is a two-dimensional 
problem: jobs request a certain number of nodes for a certain duration. As power-limited
computing becomes more common, schedulers will add power bounds to this mix: a job will
be allowed nodes, time, and a certain number of watts (the responsibility for not exceeding
the job power bound rests with the system software, not the user or application). The 
scheduler not only determines when jobs in the queue begin execution, but also what happens
when a job exits the system. Depending on the priorities of already-running jobs and the
priorities of jobs in the queue, the best solution in terms of throughput may be to idle
the recently-freed nodes and redistribute the freed power to running jobs.

\item \textbf{Site level:} At the level of the machine room (or multiple machine rooms),
decisions must be made as to how much power should be allocated for cooling versus computation, which requires understanding how temperature interacts with performance. A higher intake air temperature uses less cooling power but results in higher static processor power and may limit opportunities for ``turbo'' mode in processors where it is available. As cooling power varies with outside air temperature, a single machine room temperature setpoint may not be the optimal solution in terms of overall performance. 
\end{itemize}



\section{Prior Work}
\label{sub:priorwork}
\input{priorwork}

